{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando dados da próxima corrida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m \u001b[39mimport\u001b[39;00m webdriver\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchrome\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptions\u001b[39;00m \u001b[39mimport\u001b[39;00m Options\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m london_time, establish_connection\n\u001b[1;32m     11\u001b[0m \u001b[39m# Configura as opções do Chrome para executar em modo headless\u001b[39;00m\n\u001b[1;32m     12\u001b[0m chrome_options \u001b[39m=\u001b[39m Options()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "import pandas as pd\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.remote.webdriver import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from utils import london_time, establish_connection\n",
    "\n",
    "\n",
    "# Configura as opções do Chrome para executar em modo headless\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "url_base = \"https://www.oddschecker.com/greyhounds\"\n",
    "pattern = r\"/greyhounds/[a-zA-Z-]+/\\d{2}:\\d{2}/winner\"\n",
    "top_2_finish = \"top-2-finish\"\n",
    "top_3_finish = \"top-3-finish\"\n",
    "\n",
    "# Query\n",
    "query = \"INSERT INTO odds (odd, nome_pista, quando, trap, nome_galgo, mercado) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado) DO UPDATE SET odd = excluded.odd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cria o scraper\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# Busca as corridas da página\n",
    "request = scraper.get(url_base)\n",
    "request = request.text\n",
    "# request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(request, \"html.parser\")\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constrói um dataframe com as corridas do dia\n",
    "def get_race_list(soup):\n",
    "    # Verificar se a tag 'html' contém o atributo 'ng-app=\"ocAngularApp\"'\n",
    "    if soup.html.has_attr(\"ng-app\") and soup.html[\"ng-app\"] == \"ocAngularApp\":\n",
    "        # print(\"O atributo ng-app='ocAngularApp' existe.\")\n",
    "        races = soup.find_all(\n",
    "            \"li\", class_=\"group accordian-parent beta-body\", attrs={\"data-day\": True}\n",
    "        )\n",
    "\n",
    "        for race in races:\n",
    "            races = race.find_all(\"a\")\n",
    "            for link in races:\n",
    "                link = link.get(\"href\")\n",
    "                if re.match(pattern, link):\n",
    "                    link = \"https://www.oddschecker.com\" + link\n",
    "                    data.append(link)\n",
    "\n",
    "    else:\n",
    "        # print(\"O atributo ng-app='ocAngularApp' não existe.\\n\")\n",
    "        def find_race_meets_container(tag):\n",
    "            return (\n",
    "                tag.name == \"div\"\n",
    "                and tag.has_attr(\"class\")\n",
    "                and \"race-meets-container\" in tag[\"class\"]\n",
    "            )\n",
    "\n",
    "        uk_container = soup.find(find_race_meets_container)\n",
    "\n",
    "        races = uk_container.find_all(\"div\", class_=\"race-details\")\n",
    "\n",
    "        for race in races:\n",
    "            races = race.find_all(\"a\")\n",
    "            for link in races:\n",
    "                link = link.get(\"href\")\n",
    "                if re.match(pattern, link):\n",
    "                    link = \"https://www.oddschecker.com\" + link\n",
    "                    data.append(link)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"link\"])\n",
    "    df[\"lugar\"] = df[\"link\"].apply(\n",
    "        lambda link: re.search(r\"/greyhounds/([^/]+)/\\d{2}:\\d{2}/\", link).group(1)\n",
    "    )\n",
    "    df[\"quando\"] = df[\"link\"].apply(\n",
    "        lambda link: re.search(r\"/\\d{2}:\\d{2}/\", link).group().strip(\"/\")\n",
    "    )\n",
    "    df[\"quando\"] = pd.to_datetime(df[\"quando\"])\n",
    "    df[\"mercado\"] = df[\"link\"].apply(\n",
    "        lambda link: re.search(r\"/winner\", link).group().strip(\"/\")\n",
    "    )\n",
    "    race_list = df.sort_values(\"quando\")\n",
    "\n",
    "    return race_list\n",
    "\n",
    "# Busca o link da próxima corrida no horário londrino\n",
    "def get_next_race(df):\n",
    "    next_race = df[df[\"quando\"] > london_time()].head(1)\n",
    "    next_race = next_race.iloc[0][\"link\"]\n",
    "    print(\"\\nMonitorando a próxima corrida:\\n\", next_race, \"\\n\")\n",
    "\n",
    "    return next_race\n",
    "\n",
    "# Busca as próximas corridas nos próximos x minutos\n",
    "def get_upcoming_races(df, minutes):\n",
    "    # Calcula o tempo limite (x minutos a partir da hora atual)\n",
    "    time_limit = london_time() + timedelta(minutes=minutes)\n",
    "\n",
    "    # Filtra as corridas cujo horário esteja dentro do intervalo\n",
    "    upcoming_races = df[(df[\"quando\"] > london_time()) & (df[\"quando\"] <= time_limit)]\n",
    "    upcoming_races_links = upcoming_races[\"link\"].tolist()\n",
    "\n",
    "    print(\"\\n\", upcoming_races_links, \"\\n\")\n",
    "\n",
    "    return upcoming_races_links\n",
    "\n",
    "# Pega as odds da corrida\n",
    "def get_data_races(race):\n",
    "    # Extraindo dados da URL\n",
    "    url_parts = race.split(\"/\")\n",
    "\n",
    "    onde = url_parts[4]\n",
    "    quando = url_parts[5]\n",
    "    mercado = url_parts[6]\n",
    "    date = datetime.now().date()\n",
    "    quando = f\"{date} {quando}\"\n",
    "\n",
    "    try:\n",
    "        request = scraper.get(race)\n",
    "        request = request.text\n",
    "\n",
    "        soup = BeautifulSoup(request, \"html.parser\")\n",
    "\n",
    "        if soup.html.has_attr(\"ng-app\") and soup.html[\"ng-app\"] == \"ocAngularApp\":\n",
    "            print(\"pega por lista <li>\")\n",
    "            # SITUAÇÃO 2\n",
    "            driver = uc.Chrome()\n",
    "            driver.get(race)\n",
    "\n",
    "            # click on popup\n",
    "            time.sleep(5)\n",
    "            driver.find_element(\n",
    "                By.XPATH, \"/html/body/div[2]/div/div/div/button[1]\"\n",
    "            ).click()\n",
    "\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # # consulte o html\n",
    "            # with open(\"race_list_list.html\", \"w\") as file:\n",
    "            #     file.write(soup.prettify())\n",
    "\n",
    "            # Encontrar o tbody desejado pelo id\n",
    "            tbody = soup.find(\"tbody\", id=\"t1\")\n",
    "\n",
    "            data = []\n",
    "\n",
    "            # Iterar sobre todos os elementos tr dentro do tbody\n",
    "            for tr in tbody.find_all(\"tr\", class_=\"diff-row evTabRow bc\"):\n",
    "                trap = tr.find(\"td\", class_=\"trap-cell\").text\n",
    "                galgo = tr.find(\"td\", class_=\"sel nm basket-active\").text\n",
    "                odds = tr.find(\"td\", class_=\"bc\")\n",
    "                odd_frac = odds[\"data-o\"]\n",
    "                odd_dec = odds[\"data-odig\"]\n",
    "                data_fodds = odds[\"data-fodds\"]\n",
    "\n",
    "                # data.append([trap, galgo, odd_dec, onde, quando, mercado])\n",
    "                data.append([trap, galgo, odd_dec, odd_frac, data_fodds, onde, quando, mercado])\n",
    "\n",
    "                conn = establish_connection()\n",
    "                cur = conn.cursor()\n",
    "                query = \"INSERT INTO odds (odd, nome_pista, quando, trap, nome_galgo, mercado) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado) DO UPDATE SET odd = excluded.odd\"\n",
    "                data_sql = (odd_dec, onde, quando, trap, galgo, mercado)\n",
    "                cur.execute(query, data_sql)\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "\n",
    "            driver.quit()\n",
    "        else:\n",
    "            print(\"pega por div <div>\")\n",
    "            # SITUAÇÃO 1\n",
    "\n",
    "            dog_list = soup.find_all(\"tr\", class_=\"diff-row evTabRow bc\")\n",
    "\n",
    "            data = []\n",
    "\n",
    "            for dog in dog_list:\n",
    "                trap = dog.find(\"span\", class_=\"trap\").text\n",
    "                galgo = dog.find(\"a\", class_=\"popup selTxt\").text\n",
    "                odds = dog.find(\"td\", class_=\"bc\")\n",
    "                odd_frac = odds[\"data-o\"]\n",
    "                odd_dec = odds[\"data-odig\"]\n",
    "                data_fodds = odds[\"data-fodds\"]\n",
    "\n",
    "                data.append(\n",
    "                    [trap, galgo, odd_dec, odd_frac, data_fodds, onde, quando, mercado]\n",
    "                )\n",
    "\n",
    "                conn = establish_connection()\n",
    "                cur = conn.cursor()\n",
    "                query = \"INSERT INTO odds (odd, nome_pista, quando, trap, nome_galgo, mercado) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado) DO UPDATE SET odd = excluded.odd\"\n",
    "                data_sql = (odd_dec, onde, quando, trap, galgo, mercado)\n",
    "                cur.execute(query, data_sql)\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "\n",
    "            # print(data)\n",
    "        try:\n",
    "            df = pd.DataFrame(\n",
    "                data,\n",
    "                columns=[\n",
    "                    \"trap\",\n",
    "                    \"galgo\",\n",
    "                    \"odd_dec\",\n",
    "                    \"odd_frac\",\n",
    "                    \"data_fodds\",\n",
    "                    \"onde\",\n",
    "                    \"quando\",\n",
    "                    \"mercado\",\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            if not os.path.isfile('dados.csv'):\n",
    "                df.to_csv('dados.csv', index=False)\n",
    "            else:\n",
    "                df_existing = pd.read_csv('dados.csv')\n",
    "                df_updated = pd.concat([df_existing, df], ignore_index=True)\n",
    "                df_updated.to_csv('dados.csv', index=False)\n",
    "                \n",
    "        except:\n",
    "            print('Erro ao criar csv')\n",
    "\n",
    "    except scraper.simpleException as e:\n",
    "        print(e)\n",
    "\n",
    "    print(df)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "race_list = get_race_list(soup)\n",
    "print(\"\\n Total de corridas hoje:\", get_race_list.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        next_race = get_next_race(race_list)\n",
    "        df_winner = get_data_races(next_race)\n",
    "\n",
    "        top_2_finish_nxr = next_race.replace(\"winner\", top_2_finish)\n",
    "        top_3_finish_nxr = next_race.replace(\"winner\", top_3_finish)\n",
    "\n",
    "        top_2_finish_nxr = get_data_races(top_2_finish_nxr)\n",
    "        top_3_finish_nxr = get_data_races(top_3_finish_nxr)\n",
    "\n",
    "        time.sleep(30)\n",
    "\n",
    "    except:\n",
    "        print(\"Erro ao tentar capturar a próxima corrida\")\n",
    "        print(\"Provavelmente elas terminaram...\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
