{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando próximas corridas em função do tempo definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O atributo ng-app='ocAngularApp' não existe.\n",
      "\n",
      "Total de corridas hoje: 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_830074/4052952628.py:76: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['quando'] = pd.to_datetime(df['quando'])\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "import re\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from selenium.webdriver.remote.webdriver import By\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "url_base = \"https://www.oddschecker.com/greyhounds\"\n",
    "pattern = r\"/greyhounds/[a-zA-Z-]+/\\d{2}:\\d{2}/winner\"\n",
    "top_2_finish = 'top-2-finish'\n",
    "top_3_finish = 'top-3-finish'\n",
    "\n",
    "data = []\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "# print(scraper.get(url_base).content)\n",
    "\n",
    "request = scraper.get(url_base)\n",
    "request = request.text\n",
    "request\n",
    "\n",
    "soup = BeautifulSoup(request, \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "def establish_connection():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"galgos\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "def race_list(soup):\n",
    "    # Verificar se a tag 'html' contém o atributo 'ng-app=\"ocAngularApp\"'\n",
    "    if soup.html.has_attr('ng-app') and soup.html['ng-app'] == 'ocAngularApp':\n",
    "        print(\"O atributo ng-app='ocAngularApp' existe.\")\n",
    "\n",
    "        races = soup.find_all('li', class_='group accordian-parent beta-body', attrs={'data-day': True})\n",
    "\n",
    "        for race in races:\n",
    "            races = race.find_all('a')\n",
    "            for link in races:\n",
    "                link = link.get('href')\n",
    "                if re.match(pattern, link):\n",
    "                    link = \"https://www.oddschecker.com\" + link\n",
    "                    data.append(link)\n",
    "\n",
    "    else:\n",
    "        print(\"O atributo ng-app='ocAngularApp' não existe.\\n\")\n",
    "\n",
    "        def find_race_meets_container(tag):\n",
    "            return tag.name == 'div' and tag.has_attr('class') and 'race-meets-container' in tag['class']\n",
    "\n",
    "        uk_container = soup.find(find_race_meets_container)\n",
    "\n",
    "        races = uk_container.find_all('div', class_='race-details')\n",
    "\n",
    "        for race in races:\n",
    "            races = race.find_all('a')\n",
    "            for link in races:\n",
    "                    link = link.get('href')\n",
    "                    if re.match(pattern, link):\n",
    "                            link = \"https://www.oddschecker.com\" + link\n",
    "                            data.append(link)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['link'])\n",
    "    df['lugar'] = df['link'].apply(lambda link: re.search(r'/greyhounds/([^/]+)/\\d{2}:\\d{2}/', link).group(1))\n",
    "    df['quando'] = df['link'].apply(lambda link: re.search(r'/\\d{2}:\\d{2}/', link).group().strip('/'))\n",
    "    df['quando'] = pd.to_datetime(df['quando'])\n",
    "    df['mercado'] = df['link'].apply(lambda link: re.search(r'/winner', link).group().strip('/'))\n",
    "    race_list = df.sort_values('quando')\n",
    "    \n",
    "    return race_list\n",
    "\n",
    "def get_nextrace(df):\n",
    "    #  o fuso horário de Londres\n",
    "    time_zone_uk = tz.gettz('Europe/London')\n",
    "    \n",
    "    # hora atual no fuso horário de Londres\n",
    "    current_time = datetime.now(time_zone_uk)\n",
    "    current_time = current_time.replace(tzinfo=None)\n",
    "    next_race = df[df['quando']> current_time].head(1)\n",
    "    next_race = next_race.iloc[0]['link']\n",
    "    print(next_race)\n",
    "    \n",
    "    return next_race\n",
    "\n",
    "def get_upcoming_races(df, minutes):\n",
    "    # Fuso horário de Londres\n",
    "    time_zone_uk = tz.gettz('Europe/London')\n",
    "\n",
    "    # # Hora atual no fuso horário de Londres\n",
    "    current_time = datetime.now(tz=time_zone_uk)\n",
    "    current_time = current_time.replace(tzinfo=None)\n",
    "\n",
    "    # # Calcula o tempo limite (x minutos a partir da hora atual)\n",
    "    time_limit = current_time + timedelta(minutes=minutes)\n",
    "\n",
    "    # Filtra as corridas cujo horário esteja dentro do intervalo\n",
    "    upcoming_races = df[(df['quando'] > current_time) & (df['quando'] <= time_limit)]\n",
    "    upcoming_races_links = upcoming_races['link'].tolist()\n",
    "\n",
    "    print(upcoming_races_links)\n",
    "\n",
    "    return upcoming_races_links\n",
    "\n",
    "def get_data_races(race):    \n",
    "    # Extraindo dados da URL\n",
    "    url_parts = race.split(\"/\")\n",
    "\n",
    "    onde = url_parts[4]\n",
    "    quando = url_parts[5]\n",
    "    mercado = url_parts[6]\n",
    "    date = datetime.now().date()\n",
    "    quando = f\"{date} {quando}\"\n",
    "    \n",
    "\n",
    "    try:\n",
    "        request = scraper.get(race)\n",
    "        request = request.text\n",
    "\n",
    "\n",
    "        soup = BeautifulSoup(request, \"html.parser\")\n",
    "\n",
    "        if soup.html.has_attr('ng-app') and soup.html['ng-app'] == 'ocAngularApp':\n",
    "            print('pega por lista')\n",
    "            # SITUAÇÃO 2\n",
    "            # Aqui não consigo pegar as odds em decimal\n",
    "            driver = uc.Chrome()\n",
    "            driver.get(race)\n",
    "\n",
    "            # click on popup\n",
    "            time.sleep(5)\n",
    "            driver.find_element(By.XPATH, '/html/body/div[2]/div/div/div/button[1]').click()\n",
    "\n",
    "            # Obter o HTML da página\n",
    "            html_content = driver.page_source\n",
    "\n",
    "            # driver.quit()\n",
    "\n",
    "            # Criar o objeto BeautifulSoup para analisar o HTML\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # # consulte o html\n",
    "            with open('race_list_list.html', 'w') as file:\n",
    "                file.write(soup.prettify())\n",
    "            \n",
    "            # Encontrar o tbody desejado pelo id\n",
    "            tbody = soup.find(\"tbody\", id=\"t1\")\n",
    "\n",
    "            data = []\n",
    "\n",
    "            # Iterar sobre todos os elementos tr dentro do tbody\n",
    "            for tr in tbody.find_all(\"tr\", class_=\"diff-row evTabRow bc\"):\n",
    "                # Realizar alguma ação com cada elemento tr, por exemplo, extrair o texto\n",
    "                trap = tr.find('td', class_=\"trap-cell\").text\n",
    "                galgo = tr.find('td', class_=\"sel nm basket-active\").text\n",
    "                odds = tr.find('td', class_=\"bc\")\n",
    "                odd_frac = odds['data-o']\n",
    "                odd_dec = odds['data-odig']\n",
    "                data_fodds = odds['data-fodds']\n",
    "\n",
    "                data.append([trap,galgo, odd_dec, onde, quando, mercado])\n",
    "                \n",
    "                conn = establish_connection()\n",
    "                cur = conn.cursor()\n",
    "                query = \"INSERT INTO odds (odd, nome_pista, quando, trap, nome_galgo, mercado) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado) DO UPDATE SET odd = excluded.odd\"\n",
    "                data_sql = (odd_dec, onde, quando, trap, galgo, mercado)\n",
    "                cur.execute(query, data_sql)\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "                \n",
    "            driver.quit()\n",
    "        else:\n",
    "            print('pega por div')\n",
    "            # SITUAÇÃO 1\n",
    "\n",
    "            dog_list = soup.find_all('tr', class_='diff-row evTabRow bc')\n",
    "\n",
    "            data = []\n",
    "\n",
    "            for dog in dog_list:\n",
    "                trap = dog.find('span', class_='trap').text\n",
    "                galgo = dog.find('a', class_=\"popup selTxt\").text\n",
    "                odds = dog.find('td', class_=\"bc\")\n",
    "                odd_frac = odds['data-o']\n",
    "                odd_dec = odds['data-odig']\n",
    "                data_fodds = odds['data-fodds']\n",
    "\n",
    "                data.append([trap, galgo, odd_dec, odd_frac, data_fodds, onde, quando, mercado])\n",
    "                \n",
    "                conn = establish_connection()\n",
    "                cur = conn.cursor()\n",
    "                # query = \"INSERT INTO odds (odd, nome_pista, quando, trap, nome_galgo, mercado) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado) DO UPDATE SET odd = excluded.odd\"\n",
    "                query = \"\"\"\n",
    "                            INSERT INTO odds_teste (odd, nome_pista, quando, trap, nome_galgo, mercado)\n",
    "                            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                            ON CONFLICT (nome_pista, quando, trap, nome_galgo, mercado)\n",
    "                            DO UPDATE SET odd = EXCLUDED.odd;\n",
    "                        \"\"\"\n",
    "                data_sql = (odd_dec, onde, quando, trap, galgo, mercado)\n",
    "                cur.execute(query, data_sql)\n",
    "                conn.commit()\n",
    "                cur.close()\n",
    "                conn.close()\n",
    "\n",
    "\n",
    "            # print(data)\n",
    "        try:\n",
    "                \n",
    "            df = pd.DataFrame(data, columns=['trap', 'galgo', 'odd_dec', 'odd_frac', 'data_fodds', 'onde', 'quando', 'mercado'])\n",
    "        except:\n",
    "            df = pd.DataFrame(data, columns=['trap', 'galgo', 'odd_dec', 'onde', 'quando', 'mercado'])\n",
    "            \n",
    "    except scraper.simpleException as e:\n",
    "        print(e)\n",
    "        \n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "race_list = race_list(soup)\n",
    "print('Total de corridas hoje:',race_list.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.oddschecker.com/greyhounds/doncaster/20:17/winner', 'https://www.oddschecker.com/greyhounds/nottingham/20:21/winner', 'https://www.oddschecker.com/greyhounds/oxford/20:23/winner', 'https://www.oddschecker.com/greyhounds/youghal/20:26/winner', 'https://www.oddschecker.com/greyhounds/yarmouth/20:27/winner', 'https://www.oddschecker.com/greyhounds/nottingham/20:36/winner', 'https://www.oddschecker.com/greyhounds/doncaster/20:38/winner', 'https://www.oddschecker.com/greyhounds/oxford/20:43/winner', 'https://www.oddschecker.com/greyhounds/yarmouth/20:46/winner', 'https://www.oddschecker.com/greyhounds/nottingham/20:52/winner', 'https://www.oddschecker.com/greyhounds/doncaster/20:54/winner', 'https://www.oddschecker.com/greyhounds/yarmouth/21:01/winner', 'https://www.oddschecker.com/greyhounds/oxford/21:02/winner', 'https://www.oddschecker.com/greyhounds/nottingham/21:11/winner', 'https://www.oddschecker.com/greyhounds/doncaster/21:12/winner']\n",
      "Existem 15 corridas nos próximos 60 minutos\n",
      "https://www.oddschecker.com/greyhounds/doncaster/20:17/winner\n",
      "https://www.oddschecker.com/greyhounds/doncaster/20:17/top-2-finish\n",
      "https://www.oddschecker.com/greyhounds/doncaster/20:17/top-3-finish\n",
      "pega por div\n",
      "  trap           galgo odd_dec odd_frac data_fodds       onde  \\\n",
      "0    6   Pesti Pestana    2.75      7/4        3.0  doncaster   \n",
      "1    4    Da Real Deal       4        3       4.33  doncaster   \n",
      "2    3     Vale Blonde     4.5      7/2        6.0  doncaster   \n",
      "3    2    Ryecroft Nyx     5.5      9/2        4.0  doncaster   \n",
      "4    5    Pennys Seven     6.5     11/2       4.33  doncaster   \n",
      "5    1  Jumeirah Smile      17       16       11.0  doncaster   \n",
      "\n",
      "             quando mercado  \n",
      "0  2023-06-05 20:17  winner  \n",
      "1  2023-06-05 20:17  winner  \n",
      "2  2023-06-05 20:17  winner  \n",
      "3  2023-06-05 20:17  winner  \n",
      "4  2023-06-05 20:17  winner  \n",
      "5  2023-06-05 20:17  winner  \n",
      "pega por div\n",
      "  trap           galgo odd_dec odd_frac data_fodds       onde  \\\n",
      "0    6   Pesti Pestana    1.57      4/7       1.67  doncaster   \n",
      "1    4    Da Real Deal       2        1       2.38  doncaster   \n",
      "2    3     Vale Blonde     2.2      6/5       2.75  doncaster   \n",
      "3    2    Ryecroft Nyx    2.63     13/8        2.1  doncaster   \n",
      "4    5    Pennys Seven       3        2       2.38  doncaster   \n",
      "5    1  Jumeirah Smile     6.5     11/2        5.0  doncaster   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:17  top-2-finish  \n",
      "1  2023-06-05 20:17  top-2-finish  \n",
      "2  2023-06-05 20:17  top-2-finish  \n",
      "3  2023-06-05 20:17  top-2-finish  \n",
      "4  2023-06-05 20:17  top-2-finish  \n",
      "5  2023-06-05 20:17  top-2-finish  \n",
      "pega por div\n",
      "  trap           galgo odd_dec odd_frac data_fodds       onde  \\\n",
      "0    6   Pesti Pestana    1.22      2/9       1.29  doncaster   \n",
      "1    4    Da Real Deal    1.44      4/9       1.67  doncaster   \n",
      "2    3     Vale Blonde    1.53     8/15       1.83  doncaster   \n",
      "3    2    Ryecroft Nyx    1.73     8/11        1.5  doncaster   \n",
      "4    5    Pennys Seven    1.91    10/11       1.67  doncaster   \n",
      "5    1  Jumeirah Smile     3.5      5/2       2.75  doncaster   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:17  top-3-finish  \n",
      "1  2023-06-05 20:17  top-3-finish  \n",
      "2  2023-06-05 20:17  top-3-finish  \n",
      "3  2023-06-05 20:17  top-3-finish  \n",
      "4  2023-06-05 20:17  top-3-finish  \n",
      "5  2023-06-05 20:17  top-3-finish  \n",
      "Número de iterações: 1\n",
      "https://www.oddschecker.com/greyhounds/nottingham/20:21/winner\n",
      "https://www.oddschecker.com/greyhounds/nottingham/20:21/top-2-finish\n",
      "https://www.oddschecker.com/greyhounds/nottingham/20:21/top-3-finish\n",
      "pega por div\n",
      "  trap            galgo odd_dec odd_frac data_fodds        onde  \\\n",
      "0    4    Sambar Seamus    2.25      5/4       2.75  nottingham   \n",
      "1    1        Easy Boss     3.5      5/2        3.5  nottingham   \n",
      "2    6      One Day Ray       8        7        8.0  nottingham   \n",
      "3    3      Glimmer Man     8.5     15/2        8.5  nottingham   \n",
      "4    5     Aussie Mccoy       8        7        7.0  nottingham   \n",
      "5    2  Noduff New York       6        5        4.5  nottingham   \n",
      "\n",
      "             quando mercado  \n",
      "0  2023-06-05 20:21  winner  \n",
      "1  2023-06-05 20:21  winner  \n",
      "2  2023-06-05 20:21  winner  \n",
      "3  2023-06-05 20:21  winner  \n",
      "4  2023-06-05 20:21  winner  \n",
      "5  2023-06-05 20:21  winner  \n",
      "pega por div\n",
      "  trap            galgo odd_dec odd_frac data_fodds        onde  \\\n",
      "0    4    Sambar Seamus     1.4      2/5       1.57  nottingham   \n",
      "1    1        Easy Boss    1.83      5/6       1.91  nottingham   \n",
      "2    2  Noduff New York    2.75      7/4       2.25  nottingham   \n",
      "3    5     Aussie Mccoy     3.5      5/2       3.25  nottingham   \n",
      "4    6      One Day Ray     3.5      5/2       3.75  nottingham   \n",
      "5    3      Glimmer Man    3.75     11/4       3.75  nottingham   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:21  top-2-finish  \n",
      "1  2023-06-05 20:21  top-2-finish  \n",
      "2  2023-06-05 20:21  top-2-finish  \n",
      "3  2023-06-05 20:21  top-2-finish  \n",
      "4  2023-06-05 20:21  top-2-finish  \n",
      "5  2023-06-05 20:21  top-2-finish  \n",
      "pega por div\n",
      "  trap            galgo odd_dec odd_frac data_fodds        onde  \\\n",
      "0    4    Sambar Seamus    1.14      1/7       1.22  nottingham   \n",
      "1    1        Easy Boss    1.36     4/11        1.4  nottingham   \n",
      "2    2  Noduff New York     1.8      4/5       1.57  nottingham   \n",
      "3    5     Aussie Mccoy     2.1    11/10        2.0  nottingham   \n",
      "4    6      One Day Ray     2.1    11/10       2.25  nottingham   \n",
      "5    3      Glimmer Man    2.25      5/4       2.25  nottingham   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:21  top-3-finish  \n",
      "1  2023-06-05 20:21  top-3-finish  \n",
      "2  2023-06-05 20:21  top-3-finish  \n",
      "3  2023-06-05 20:21  top-3-finish  \n",
      "4  2023-06-05 20:21  top-3-finish  \n",
      "5  2023-06-05 20:21  top-3-finish  \n",
      "Número de iterações: 2\n",
      "https://www.oddschecker.com/greyhounds/oxford/20:23/winner\n",
      "https://www.oddschecker.com/greyhounds/oxford/20:23/top-2-finish\n",
      "https://www.oddschecker.com/greyhounds/oxford/20:23/top-3-finish\n",
      "pega por div\n",
      "  trap             galgo odd_dec odd_frac data_fodds    onde  \\\n",
      "0    5   Moneygall Rocky    3.25      9/4       3.25  oxford   \n",
      "1    2  Ballintemple Lou    3.75     11/4        3.5  oxford   \n",
      "2    1    Farloe Caprice    3.75     11/4        3.5  oxford   \n",
      "3    3      Farloe Elton       4        3        3.5  oxford   \n",
      "4    4       Zari Millie       7        6        8.0  oxford   \n",
      "\n",
      "             quando mercado  \n",
      "0  2023-06-05 20:23  winner  \n",
      "1  2023-06-05 20:23  winner  \n",
      "2  2023-06-05 20:23  winner  \n",
      "3  2023-06-05 20:23  winner  \n",
      "4  2023-06-05 20:23  winner  \n",
      "pega por div\n",
      "  trap             galgo odd_dec odd_frac data_fodds    onde  \\\n",
      "0    5   Moneygall Rocky    1.73     8/11       1.73  oxford   \n",
      "1    2  Ballintemple Lou    1.91    10/11        1.8  oxford   \n",
      "2    1    Farloe Caprice    1.91    10/11        2.0  oxford   \n",
      "3    3      Farloe Elton       2        1        2.0  oxford   \n",
      "4    4       Zari Millie       3        2        3.4  oxford   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:23  top-2-finish  \n",
      "1  2023-06-05 20:23  top-2-finish  \n",
      "2  2023-06-05 20:23  top-2-finish  \n",
      "3  2023-06-05 20:23  top-2-finish  \n",
      "4  2023-06-05 20:23  top-2-finish  \n",
      "pega por div\n",
      "  trap             galgo odd_dec odd_frac data_fodds    onde  \\\n",
      "0    5   Moneygall Rocky    1.25      1/4       1.25  oxford   \n",
      "1    1    Farloe Caprice    1.33      1/3       1.36  oxford   \n",
      "2    2  Ballintemple Lou    1.33      1/3       1.29  oxford   \n",
      "3    3      Farloe Elton    1.36     4/11       1.36  oxford   \n",
      "4    4       Zari Millie    1.83      5/6        2.0  oxford   \n",
      "\n",
      "             quando       mercado  \n",
      "0  2023-06-05 20:23  top-3-finish  \n",
      "1  2023-06-05 20:23  top-3-finish  \n",
      "2  2023-06-05 20:23  top-3-finish  \n",
      "3  2023-06-05 20:23  top-3-finish  \n",
      "4  2023-06-05 20:23  top-3-finish  \n",
      "Número de iterações: 3\n",
      "https://www.oddschecker.com/greyhounds/youghal/20:26/winner\n",
      "https://www.oddschecker.com/greyhounds/youghal/20:26/top-2-finish\n",
      "https://www.oddschecker.com/greyhounds/youghal/20:26/top-3-finish\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "catching classes that do not inherit from BaseException is not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m, in \u001b[0;36mget_data_races\u001b[0;34m(race)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     request \u001b[39m=\u001b[39m scraper\u001b[39m.\u001b[39;49mget(race)\n\u001b[1;32m    127\u001b[0m     request \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/cloudscraper/__init__.py:259\u001b[0m, in \u001b[0;36mCloudScraper.request\u001b[0;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# Make the request via requests.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m    258\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodeBrotli(\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(method, url, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m# Debug the request via the Response object.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# ------------------------------------------------------------------------------- #\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/cloudscraper/__init__.py:192\u001b[0m, in \u001b[0;36mCloudScraper.perform_request\u001b[0;34m(self, method, url, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\u001b[39mself\u001b[39m, method, url, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(CloudScraper, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/requests/sessions.py:719\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, resp\u001b[39m.\u001b[39mrequest, resp\u001b[39m.\u001b[39mraw)\n\u001b[0;32m--> 719\u001b[0m extract_cookies_to_jar(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcookies, request, r\u001b[39m.\u001b[39;49mraw)\n\u001b[1;32m    721\u001b[0m \u001b[39m# Resolve redirects if allowed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/workana/corridas/venv/lib/python3.11/site-packages/requests/cookies.py:137\u001b[0m, in \u001b[0;36mextract_cookies_to_jar\u001b[0;34m(jar, request, response)\u001b[0m\n\u001b[1;32m    136\u001b[0m res \u001b[39m=\u001b[39m MockResponse(response\u001b[39m.\u001b[39m_original_response\u001b[39m.\u001b[39mmsg)\n\u001b[0;32m--> 137\u001b[0m jar\u001b[39m.\u001b[39;49mextract_cookies(res, req)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/http/cookiejar.py:1686\u001b[0m, in \u001b[0;36mCookieJar.extract_cookies\u001b[0;34m(self, response, request)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39mfor\u001b[39;00m cookie \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_cookies(response, request):\n\u001b[0;32m-> 1686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy\u001b[39m.\u001b[39;49mset_ok(cookie, request):\n\u001b[1;32m   1687\u001b[0m         _debug(\u001b[39m\"\u001b[39m\u001b[39m setting cookie: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, cookie)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/http/cookiejar.py:963\u001b[0m, in \u001b[0;36mDefaultCookiePolicy.set_ok\u001b[0;34m(self, cookie, request)\u001b[0m\n\u001b[1;32m    962\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, fn_name)\n\u001b[0;32m--> 963\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fn(cookie, request):\n\u001b[1;32m    964\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/http/cookiejar.py:984\u001b[0m, in \u001b[0;36mDefaultCookiePolicy.set_ok_verifiability\u001b[0;34m(self, cookie, request)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_ok_verifiability\u001b[39m(\u001b[39mself\u001b[39m, cookie, request):\n\u001b[0;32m--> 984\u001b[0m     \u001b[39mif\u001b[39;00m request\u001b[39m.\u001b[39munverifiable \u001b[39mand\u001b[39;00m is_third_party(request):\n\u001b[1;32m    985\u001b[0m         \u001b[39mif\u001b[39;00m cookie\u001b[39m.\u001b[39mversion \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrict_rfc2965_unverifiable:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/http/cookiejar.py:737\u001b[0m, in \u001b[0;36mis_third_party\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    736\u001b[0m req_host \u001b[39m=\u001b[39m request_host(request)\n\u001b[0;32m--> 737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m domain_match(req_host, reach(request\u001b[39m.\u001b[39;49morigin_req_host)):\n\u001b[1;32m    738\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/http/cookiejar.py:579\u001b[0m, in \u001b[0;36mdomain_match\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m i \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39;49mrfind(B)\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    581\u001b[0m     \u001b[39m# A does not have form NB, or N is the empty string\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(top_2_finish_nxr)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(top_3_finish_nxr)\n\u001b[0;32m---> 15\u001b[0m get_data_races(race)\n\u001b[1;32m     16\u001b[0m top_2_finish_nxr \u001b[39m=\u001b[39m get_data_races(top_2_finish_nxr)\n\u001b[1;32m     17\u001b[0m top_3_finish_nxr \u001b[39m=\u001b[39m get_data_races(top_3_finish_nxr)\n",
      "Cell \u001b[0;32mIn[1], line 223\u001b[0m, in \u001b[0;36mget_data_races\u001b[0;34m(race)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrap\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgalgo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39modd_dec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39monde\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mquando\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmercado\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 223\u001b[0m \u001b[39mexcept\u001b[39;00m scraper\u001b[39m.\u001b[39msimpleException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    224\u001b[0m     \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m    226\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "\u001b[0;31mTypeError\u001b[0m: catching classes that do not inherit from BaseException is not allowed"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    minutes = 60\n",
    "    next_races = get_upcoming_races(race_list, minutes)\n",
    "\n",
    "    print(f'Existem {len(next_races)} corridas nos próximos {minutes} minutos')\n",
    "\n",
    "    count = 0\n",
    "    for race in next_races:\n",
    "        print(race)\n",
    "        top_2_finish_nxr = race.replace('winner', top_2_finish)\n",
    "        top_3_finish_nxr = race.replace('winner', top_3_finish)\n",
    "        print(top_2_finish_nxr)\n",
    "        print(top_3_finish_nxr)\n",
    "\n",
    "        get_data_races(race)\n",
    "        top_2_finish_nxr = get_data_races(top_2_finish_nxr)\n",
    "        top_3_finish_nxr = get_data_races(top_3_finish_nxr)\n",
    "        count += 1\n",
    "        print(\"Número de iterações:\", count)\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "while True:\n",
    "    # Cria threads para extrair dados das páginas\n",
    "    threads = []\n",
    "    for url in next_races:\n",
    "        thread = threading.Thread(target=get_data_races, args=(url,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Aguarda todas as threads terminarem\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_race = get_data_races('https://www.oddschecker.com/greyhounds/kinsley/17:14/winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes=60\n",
    "\n",
    "try:\n",
    "    next_races = get_upcoming_races(race_list,minutes)\n",
    "    xxx = get_data_races(next_race)\n",
    "    \n",
    "    # top_2_finish_nxr = next_race.replace('winner', top_2_finish)\n",
    "    # top_3_finish_nxr = next_race.replace('winner', top_3_finish)\n",
    "\n",
    "    # top_2_finish_nxr = get_data_races(top_2_finish_nxr)\n",
    "    # top_3_finish_nxr = get_data_races(top_3_finish_nxr)\n",
    "except:\n",
    "    print('Erro ao tentar capturar a próxima corrida')\n",
    "    print('Provavelmente elas terminaram...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2_finish_nxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # APENAS TESTE NO BRASIL\n",
    "# CASO ESTEJA VENDO UM DIA ANTERIOR AO DAS CORRIDAS\n",
    "# EM PRODUÇÃO ESSE DF DEVE SER CONSTRUIDO NA VIRADA DE UM DIA PARA OUTRO\n",
    "\n",
    "# Obtendo a data atual\n",
    "# current_date = datetime.now().date()\n",
    "\n",
    "# Adicionando um dia à data atual para indicar que é amanhã\n",
    "# target_date = current_date + timedelta(days=1)\n",
    "\n",
    "# Convertendo a coluna 'Hora' para o formato datetime, especificando a data como amanhã\n",
    "# df['quando'] = pd.to_datetime(df['quando'], format='%Y-%m-%d %H:%M') + pd.DateOffset(days=1)\n",
    "# df\n",
    "\n",
    "# Pega a próxima corrida no fuso horário londrino\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
